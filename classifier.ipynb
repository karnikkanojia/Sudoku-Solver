{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Conv2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Datapoints: 10160\n"
     ]
    }
   ],
   "source": [
    "data = os.listdir(\"Digits\")\n",
    "data_X, data_Y = [], []\n",
    "num_classes = len(data)\n",
    "for i in range(num_classes):\n",
    "    data_list = os.listdir(f\"Digits/{i}\")\n",
    "    for j in data_list:\n",
    "        img = cv2.imread(f\"Digits/{i}/{j}\")\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        data_X.append(img)\n",
    "        data_Y.append(i)\n",
    "if len(data_X) == len(data_Y):\n",
    "    print(f\"Total Datapoints: {len(data_X)}\")\n",
    "else: print(\"Not all data extracted\")\n",
    "data_X = np.array(data_X)\n",
    "data_Y = np.array(data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size = 7721\n",
      "Validation Size = 1931\n",
      "Test Size = 508\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(data_X, data_Y, test_size=0.05, random_state=42)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
    "print(f\"Training Size = {len(train_X)}\")\n",
    "print(f\"Validation Size = {len(valid_X)}\")\n",
    "print(f\"Test Size = {len(test_X)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img: np.ndarray) -> np.ndarray:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.equalizeHist(img)\n",
    "    img = img/255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7721, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array(list(map(preprocess, train_X)))\n",
    "test_X = np.array(list(map(preprocess, test_X)))\n",
    "valid_X = np.array(list(map(preprocess, valid_X)))\n",
    "train_X = np.expand_dims(train_X, axis=-1)\n",
    "test_X = np.expand_dims(test_X, axis=-1)\n",
    "valid_X = np.expand_dims(valid_X, axis=-1)\n",
    "print(train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.2, shear_range=0.1, rotation_range=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7721,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = to_categorical(train_y, num_classes)\n",
    "test_y = to_categorical(test_y, num_classes)\n",
    "valid_y = to_categorical(valid_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-28 13:12:16.374978: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(60, 5, input_shape=(32, 32, 1), padding='same', activation='relu'),\n",
    "    Conv2D(60, 5, padding='same', activation='relu'),\n",
    "    MaxPool2D(2),\n",
    "    Conv2D(30, 3, padding='same', activation='relu'),\n",
    "    Conv2D(30, 3, padding='same', activation='relu'),\n",
    "    MaxPool2D(2, strides=2),\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    Dense(500, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 60)        1560      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 60)        90060     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 60)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 30)        16230     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 30)        8130      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 30)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 8, 8, 30)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               960500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,081,490\n",
      "Trainable params: 1,081,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.001, rho=0.9, epsilon = 1e-08, decay=0.0)\n",
    "model.compile(optimizer=opt,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"best_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callback = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "200/200 [==============================] - 85s 425ms/step - loss: 0.1821 - accuracy: 0.9465 - val_loss: 0.0511 - val_accuracy: 0.9819\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.98187, saving model to best_weights.hdf5\n",
      "Epoch 2/30\n",
      "200/200 [==============================] - 83s 417ms/step - loss: 0.1479 - accuracy: 0.9533 - val_loss: 0.0336 - val_accuracy: 0.9891\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.98187 to 0.98912, saving model to best_weights.hdf5\n",
      "Epoch 3/30\n",
      "200/200 [==============================] - 87s 434ms/step - loss: 0.1251 - accuracy: 0.9613 - val_loss: 0.0352 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.98912 to 0.99068, saving model to best_weights.hdf5\n",
      "Epoch 4/30\n",
      "200/200 [==============================] - 82s 411ms/step - loss: 0.1125 - accuracy: 0.9649 - val_loss: 0.0287 - val_accuracy: 0.9907\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.99068\n",
      "Epoch 5/30\n",
      "200/200 [==============================] - 82s 408ms/step - loss: 0.1011 - accuracy: 0.9713 - val_loss: 0.0319 - val_accuracy: 0.9896\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.99068\n",
      "Epoch 6/30\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.0977 - accuracy: 0.9726 - val_loss: 0.0234 - val_accuracy: 0.9927\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.99068 to 0.99275, saving model to best_weights.hdf5\n",
      "Epoch 7/30\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.0980 - accuracy: 0.9702 - val_loss: 0.0455 - val_accuracy: 0.9850\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.99275\n",
      "Epoch 8/30\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.0804 - accuracy: 0.9748 - val_loss: 0.0203 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.99275 to 0.99379, saving model to best_weights.hdf5\n",
      "Epoch 9/30\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.0949 - accuracy: 0.9719 - val_loss: 0.0184 - val_accuracy: 0.9933\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.99379\n",
      "Epoch 10/30\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.0721 - accuracy: 0.9774 - val_loss: 0.0231 - val_accuracy: 0.9917\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.99379\n",
      "Epoch 11/30\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.0912 - accuracy: 0.9747 - val_loss: 0.0332 - val_accuracy: 0.9896\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.99379\n",
      "Epoch 12/30\n",
      "200/200 [==============================] - 81s 406ms/step - loss: 0.0786 - accuracy: 0.9771 - val_loss: 0.0171 - val_accuracy: 0.9959\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.99379 to 0.99586, saving model to best_weights.hdf5\n",
      "Epoch 13/30\n",
      "200/200 [==============================] - 81s 403ms/step - loss: 0.0818 - accuracy: 0.9774 - val_loss: 0.0208 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.99586\n",
      "Epoch 14/30\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0831 - accuracy: 0.9769 - val_loss: 0.0195 - val_accuracy: 0.9959\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.99586\n",
      "Epoch 15/30\n",
      "200/200 [==============================] - 81s 405ms/step - loss: 0.0760 - accuracy: 0.9798 - val_loss: 0.0155 - val_accuracy: 0.9959\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.99586\n",
      "Epoch 16/30\n",
      "200/200 [==============================] - 81s 405ms/step - loss: 0.0668 - accuracy: 0.9796 - val_loss: 0.0291 - val_accuracy: 0.9917\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.99586\n",
      "Epoch 17/30\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0783 - accuracy: 0.9780 - val_loss: 0.0194 - val_accuracy: 0.9938\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.99586\n",
      "Epoch 18/30\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.0683 - accuracy: 0.9795 - val_loss: 0.0146 - val_accuracy: 0.9959\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.99586\n",
      "Epoch 19/30\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0672 - accuracy: 0.9815 - val_loss: 0.0199 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.99586\n",
      "Epoch 20/30\n",
      "200/200 [==============================] - 81s 406ms/step - loss: 0.0714 - accuracy: 0.9820 - val_loss: 0.0074 - val_accuracy: 0.9964\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.99586 to 0.99637, saving model to best_weights.hdf5\n",
      "Epoch 21/30\n",
      "200/200 [==============================] - 82s 410ms/step - loss: 0.0698 - accuracy: 0.9818 - val_loss: 0.0165 - val_accuracy: 0.9959\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.99637\n",
      "Epoch 22/30\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.0736 - accuracy: 0.9801 - val_loss: 0.0420 - val_accuracy: 0.9927\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.99637\n",
      "Epoch 23/30\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0695 - accuracy: 0.9799 - val_loss: 0.0111 - val_accuracy: 0.9969\n",
      "\n",
      "Epoch 00023: val_accuracy improved from 0.99637 to 0.99689, saving model to best_weights.hdf5\n",
      "Epoch 24/30\n",
      "200/200 [==============================] - 81s 404ms/step - loss: 0.0664 - accuracy: 0.9820 - val_loss: 0.0184 - val_accuracy: 0.9953\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.99689\n",
      "Epoch 25/30\n",
      "200/200 [==============================] - 82s 412ms/step - loss: 0.0633 - accuracy: 0.9837 - val_loss: 0.0397 - val_accuracy: 0.9922\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.99689\n",
      "Epoch 26/30\n",
      "200/200 [==============================] - 83s 413ms/step - loss: 0.0701 - accuracy: 0.9826 - val_loss: 0.0102 - val_accuracy: 0.9974\n",
      "\n",
      "Epoch 00026: val_accuracy improved from 0.99689 to 0.99741, saving model to best_weights.hdf5\n",
      "Epoch 27/30\n",
      "200/200 [==============================] - 82s 408ms/step - loss: 0.0663 - accuracy: 0.9820 - val_loss: 0.0156 - val_accuracy: 0.9948\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.99741\n",
      "Epoch 28/30\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 0.0649 - accuracy: 0.9826 - val_loss: 0.0091 - val_accuracy: 0.9979\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.99741 to 0.99793, saving model to best_weights.hdf5\n",
      "Epoch 29/30\n",
      "200/200 [==============================] - 82s 411ms/step - loss: 0.0562 - accuracy: 0.9879 - val_loss: 0.0112 - val_accuracy: 0.9943\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.99793\n",
      "Epoch 30/30\n",
      "200/200 [==============================] - 81s 407ms/step - loss: 0.0628 - accuracy: 0.9853 - val_loss: 0.0103 - val_accuracy: 0.9969\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.99793\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(train_X, train_y, batch_size=32), epochs = 30, validation_data = (valid_X, valid_y), verbose = 1, steps_per_epoch= 200, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 147ms/step - loss: 0.0350 - accuracy: 0.9961\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score =  0.03501947224140167\n",
      "Test Accuracy = 0.9960629940032959\n"
     ]
    }
   ],
   "source": [
    "print('Test Score = ',score[0])\n",
    "print('Test Accuracy =', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "131ef98215b326e417844fc58286905214ae65ff5ddd47715261d112bad37642"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('ml': conda)",
   "language": "python",
   "name": "python382jvsc74a57bd0131ef98215b326e417844fc58286905214ae65ff5ddd47715261d112bad37642"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
